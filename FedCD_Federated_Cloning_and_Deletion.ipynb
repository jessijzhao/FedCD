{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FedCD: Federated Cloning and Deletion",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f1ffa6da277472e92fd484a903985a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b1f1f1efc8e341b2806461c45311d968",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_14f82609002b447b9c0730bbb289400e",
              "IPY_MODEL_3d446821ef9448719dba82c723b95f89"
            ]
          }
        },
        "b1f1f1efc8e341b2806461c45311d968": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14f82609002b447b9c0730bbb289400e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1cc60d7e4c274471ab9f996d8556f28e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e99e3ad063924df1b05f2ce70b911670"
          }
        },
        "3d446821ef9448719dba82c723b95f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_856d6f14ce3c4b8f958611fe1fac9126",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:08&lt;00:00, 20118581.25it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48de9e3896184ebcba77bdde314303f6"
          }
        },
        "1cc60d7e4c274471ab9f996d8556f28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e99e3ad063924df1b05f2ce70b911670": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "856d6f14ce3c4b8f958611fe1fac9126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48de9e3896184ebcba77bdde314303f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessijzhao/FedCD/blob/master/FedCD_Federated_Cloning_and_Deletion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7ttBYeclcLI"
      },
      "source": [
        "# **FedCD: Federated Cloning and Deletion**\n",
        "\n",
        "Jessica Zhao, Eric Lin, Kavya Kopparapu \n",
        "\n",
        "Code for *FedCD: Federated Cloning and Deletion* CIFAR-10 experiments \n",
        "\n",
        "Based on code by Harvard CS 242.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5kLxSFZssRo"
      },
      "source": [
        "---\n",
        "\n",
        "### **Parameters**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8UpiDU4szfu"
      },
      "source": [
        "# setup parameters\n",
        "NUM_DEVICES = 30                                            # number of devices\n",
        "NUM_LABELS = 10                                             # number of labels\n",
        "NUM_TRAIN_PER_DEVICE = 5000                                 # number of training examples per device\n",
        "NUM_VALID_PER_DEVICE = NUM_TRAIN_PER_DEVICE // 3            # number of validation examples per device\n",
        "NUM_TEST_PER_DEVICE = 500                                   # number of test examples per device\n",
        "SAMPLER = \"uniform_bias\"                                    # data sampler\n",
        "assert (SAMPLER in [\"uniform_bias\", \"hypergeometric\"])\n",
        "\n",
        "\n",
        "# training parameters\n",
        "NUM_ROUNDS = 150                                            # number of training rounds\n",
        "NUM_ROUNDS = 2 # remove this\n",
        "NUM_LOCAL_EPOCHS = 3                                        # number of local epochs per device per round\n",
        "DEVICE_PCT = 0.5                                            # fraction of devices for each round\n",
        "\n",
        "DUPLICATE_MILESTONES = [5, 15, 25, 30]                      # training rounds at which to duplicate\n",
        "\n",
        "# quantization parameters\n",
        "QUANTIZE = True                                             # whether to quantize models\n",
        "NBIT = 8                                                    # number of bits to quantize to\n",
        "\n",
        "# which model(s) to run\n",
        "FEDCD = True                                                # Federated Cloning and Deletion\n",
        "FEDAVG = True                                               # normal federated learning algorithm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UvFA89jTuON"
      },
      "source": [
        "---\n",
        "\n",
        "### **General Setup Code**\n",
        "\n",
        "---\n",
        "Define the dataset as well as the standard net we will be using for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9WL6HA_Lpe8",
        "outputId": "30e78ebe-2a30-4ce1-84bb-2b9767b4b0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111,
          "referenced_widgets": [
            "5f1ffa6da277472e92fd484a903985a0",
            "b1f1f1efc8e341b2806461c45311d968",
            "14f82609002b447b9c0730bbb289400e",
            "3d446821ef9448719dba82c723b95f89",
            "1cc60d7e4c274471ab9f996d8556f28e",
            "e99e3ad063924df1b05f2ce70b911670",
            "856d6f14ce3c4b8f958611fe1fac9126",
            "48de9e3896184ebcba77bdde314303f6"
          ]
        }
      },
      "source": [
        "import time\n",
        "import copy\n",
        "import sys\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# Load training and validation data from CIFAR-10\n",
        "transform_train_valid = transforms.Compose([                                   \n",
        "    transforms.RandomCrop(32, padding=4),                                       \n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
        "                                        download=True,\n",
        "                                        transform=transform_train_valid)\n",
        "validset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
        "                                        download=True,\n",
        "                                        transform=transform_train_valid)\n",
        "valid_size = 0.2\n",
        "indices = list(range(len(trainset)))\n",
        "split = int(np.floor(valid_size * len(trainset)))\n",
        "    \n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, \n",
        "                                            sampler=valid_sampler, shuffle=False,\n",
        "                                            num_workers=2)\n",
        "validloader = torch.utils.data.DataLoader(validset, batch_size=128, \n",
        "                                            sampler=valid_sampler, shuffle=False,\n",
        "                                            num_workers=2)\n",
        "# Load testing data\n",
        "transform_test = transforms.Compose([                                           \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True,\n",
        "                                       transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False,\n",
        "                                         num_workers=2)\n",
        "\n",
        "\n",
        "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
        "               padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
        "                  bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            conv_block(3, 32),\n",
        "            conv_block(32, 32),\n",
        "            conv_block(32, 64, stride=2),\n",
        "            conv_block(64, 64),\n",
        "            conv_block(64, 64),\n",
        "            conv_block(64, 128, stride=2),\n",
        "            conv_block(128, 128),\n",
        "            conv_block(128, 256),\n",
        "            conv_block(256, 256),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "            )\n",
        "\n",
        "        self.classifier = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.model(x)\n",
        "        B, C, _, _ = h.shape\n",
        "        h = h.view(B, C)\n",
        "        return self.classifier(h)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f1ffa6da277472e92fd484a903985a0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjiB2IYp7B25"
      },
      "source": [
        "**Device Class and Train/Test Methods**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSx1GxV2j0iI"
      },
      "source": [
        "import statistics \n",
        "\n",
        "class DatasetSplit(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, idxs):\n",
        "        self.dataset = dataset\n",
        "        self.idxs = [int(i) for i in idxs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        image, label = self.dataset[self.idxs[item]]\n",
        "        return image, torch.tensor(label)\n",
        "\n",
        "class Device():\n",
        "    def __init__(self, net, device_id, trainset, validset, testset, train_idxs, valid_idxs, test_idxs, bias, archetype, lr=0.1,\n",
        "                      milestones=None, batch_size=128):\n",
        "        if milestones == None:\n",
        "            milestones = [25, 50, 75]\n",
        "\n",
        "        device_net = copy.deepcopy(net)\n",
        "        optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9,\n",
        "                                    weight_decay=5e-4)\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                        milestones=milestones,\n",
        "                                                        gamma=0.1)\n",
        "        self.device_trainset = DatasetSplit(trainset, train_idxs)\n",
        "        self.trainloader = torch.utils.data.DataLoader(self.device_trainset,\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        shuffle=True,\n",
        "                                                        num_workers=2)\n",
        "        self.device_validset = DatasetSplit(validset, valid_idxs)\n",
        "        self.validloader = torch.utils.data.DataLoader(self.device_validset,\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        shuffle=True,\n",
        "                                                        num_workers=2)\n",
        "        self.device_testset = DatasetSplit(testset, test_idxs)\n",
        "        self.testloader = torch.utils.data.DataLoader(self.device_testset,\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        shuffle=True,\n",
        "                                                        num_workers=2)\n",
        "        self.nets = []\n",
        "        self.idx = device_id\n",
        "\n",
        "        # values sum to 1\n",
        "        self.ranking = [1.]\n",
        "\n",
        "        # either 1 or 0, depending on whether we got rid of it or not\n",
        "        self.active = [1] \n",
        "\n",
        "        self.nets.append({\n",
        "            'net': device_net,\n",
        "            'optimizer': optimizer,\n",
        "            'scheduler': scheduler,\n",
        "            'train_loss_tracker': [],\n",
        "            'train_acc_tracker': [],\n",
        "            'valid_loss_tracker': [],\n",
        "            'valid_acc_tracker': [],\n",
        "            'test_loss_tracker': [],\n",
        "            'test_acc_tracker': [],\n",
        "        })\n",
        "\n",
        "        # Bias and archetype parameters\n",
        "        self.bias = bias              # Number between 0 and 1 to represent linear comb. of archetypes\n",
        "        self.archetype = archetype    # An array of possible archetypes\n",
        "    \n",
        "    def update_ranking(self, removed=False, duplicate_model_id=-1, offset_rank=-1):\n",
        "        # number of standard deviations away for model deletion cutoff\n",
        "        zero_threshold = 1 \n",
        "\n",
        "        if len(self.nets) > 1:\n",
        "            metrics = []\n",
        "            for i in range(len(self.nets)):\n",
        "                if(len(self.nets[i]['valid_acc_tracker']) > 0):\n",
        "                    rank = self.nets[i]['valid_acc_tracker'][-1]\n",
        "                    if(len(self.nets[i]['valid_acc_tracker']) >= 3):\n",
        "                        rank = (self.nets[i]['valid_acc_tracker'][-1]+self.nets[i]['valid_acc_tracker'][-2]+self.nets[i]['valid_acc_tracker'][-3])/3\n",
        "                    if duplicate_model_id == i and offset_rank != -1:\n",
        "                        # Heavily rank the devices that are underperforming for new models and vice versa\n",
        "                        rank = offset_rank      \n",
        "                    if rank == 0:\n",
        "                        rank += 0.001\n",
        "                    metrics.append(rank)\n",
        "                else:\n",
        "                    metrics.append(50)\n",
        "            \n",
        "            #if we added more models, add active trackers for them\n",
        "            while(len(self.nets) != len(self.active)):\n",
        "                self.active.append(1)\n",
        "            #  Auto-set a model as inactive if it was already removed\n",
        "            if removed:       \n",
        "                self.active[duplicate_model_id] = 0\n",
        "            \n",
        "\n",
        "            # normalization first time (with self.active)\n",
        "            self.ranking = [metrics[i]*self.active[i]/sum(metrics) for i in range(len(metrics))]\n",
        "\n",
        "            nonzero_elts = np.array(self.active).nonzero()[0]\n",
        "            nonzero_arr = []\n",
        "            for i in nonzero_elts:\n",
        "                nonzero_arr.append(self.ranking[i])\n",
        "            \n",
        "            # Remove models that are underperforming\n",
        "            if offset_rank == -1:   # only remove if not duplicating round\n",
        "                max_rank = max(self.ranking)\n",
        "                if len(nonzero_elts) > 1:\n",
        "                    std = statistics.stdev(nonzero_arr)\n",
        "                    mean = sum(nonzero_arr)/len(nonzero_arr)\n",
        "\n",
        "                    # remove models that are underperforming\n",
        "                    for j in range(len(self.ranking)):\n",
        "                        if self.active[j] != 0:\n",
        "                            if(mean - self.ranking[j] > zero_threshold*std):\n",
        "                                self.ranking[j] = 0\n",
        "                                self.active[j] = 0\n",
        "                            elif( len(self.ranking) > 3 and (self.ranking[j] * 10 < max_rank)):\n",
        "                                self.ranking[j] = 0\n",
        "                                self.active[j] = 0\n",
        "\n",
        "            bool_ranking_below_zero = False\n",
        "            \n",
        "            # Add noise\n",
        "            noise = random.gauss(0, 0.01)\n",
        "            if len(nonzero_elts) == 1:\n",
        "                i = nonzero_elts[0]\n",
        "            else:\n",
        "                i = nonzero_elts[random.randint(0, len(nonzero_elts)-1)]\n",
        "                for j in range(len(self.ranking)):\n",
        "                    if(j != i):\n",
        "                        self.ranking[j] -= noise/(len(nonzero_elts)-1)\n",
        "                        if self.ranking[j] < 0:\n",
        "                          bool_ranking_below_zero = True\n",
        "            \n",
        "            self.ranking[i] += noise\n",
        "            if self.ranking[i] < 0:\n",
        "                bool_ranking_below_zero = True\n",
        "\n",
        "            # Normalize again\n",
        "            if(bool_ranking_below_zero):\n",
        "                self.ranking = [self.ranking[i]-min(self.ranking) for i in range(len(self.ranking))]\n",
        "            self.ranking = [self.ranking[i]*self.active[i]/sum(self.ranking) for i in range(len(self.ranking))]\n",
        "\n",
        "            if(sum(self.active) == 2 and round_num > 15):\n",
        "                index1 = self.active.index(1)\n",
        "                index2 = self.active[index1+1:].index(1)+index1+1\n",
        "                if(self.ranking[index1] - self.ranking[index2] > 0.5):\n",
        "                    self.ranking[index2] = 0\n",
        "                    self.ranking[index1] = 1\n",
        "                    self.active[index2] = 0\n",
        "                elif(self.ranking[index2] - self.ranking[index1] > 0.5):\n",
        "                    self.ranking[index1] = 0\n",
        "                    self.ranking[index2] = 1\n",
        "                    self.active[index1] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qPw04ousTPV"
      },
      "source": [
        "\n",
        "\n",
        "def create_devices(net, trainset, validset, testset, train_idxs, valid_idxs, test_idxs, bias, archetype, lr=0.1,\n",
        "                  milestones=None, batch_size=128):\n",
        "    devices_lst = [Device(net, i, trainset, validset, testset, train_idxs[i], valid_idxs[i], test_idxs[i], bias[i], archetype[i], lr,\n",
        "                  milestones, batch_size) for i in range(NUM_DEVICES)]\n",
        "    return devices_lst\n",
        "      \n",
        "  \n",
        "def train(epoch, device, model_id):\n",
        "    device.nets[model_id]['net'].train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    dataset = device.device_trainset\n",
        "    dataloader = device.trainloader\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        device.nets[model_id]['optimizer'].zero_grad()\n",
        "        outputs = device.nets[model_id]['net'](inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        device.nets[model_id]['optimizer'].step()\n",
        "        train_loss += loss.item()\n",
        "        device.nets[model_id]['train_loss_tracker'].append(loss.item())\n",
        "        loss = train_loss / (batch_idx + 1)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        acc = 100. * correct / total\n",
        "        dev_id = device.idx\n",
        "    test_loss = 0\n",
        "    outputs = [0]\n",
        "    device.nets[model_id]['train_acc_tracker'].append(acc)\n",
        "\n",
        "def validate(epoch, device, model_id):\n",
        "    device.nets[model_id]['net'].eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    dataset = device.device_validset\n",
        "    dataloader = device.validloader\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            outputs = device.nets[model_id]['net'](inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "            device.nets[model_id]['valid_loss_tracker'].append(loss.item())\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            loss = test_loss / (batch_idx + 1)\n",
        "            acc = 100.* correct / total\n",
        "        test_loss = 0\n",
        "        outputs = [0]\n",
        "    acc = 100.*correct/total\n",
        "    device.nets[model_id]['valid_acc_tracker'].append(acc)\n",
        "    device.nets[model_id]['net'].train()\n",
        "\n",
        "def test(epoch, device, model_id, dataset, dataloader):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    device.nets[model_id]['net'].eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            outputs = device.nets[model_id]['net'](inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()            \n",
        "            device.nets[model_id]['test_loss_tracker'].append(loss.item())\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            loss = test_loss / (batch_idx + 1)\n",
        "            acc = 100.* correct / total\n",
        "        test_loss = 0\n",
        "        outputs = [0]\n",
        "    acc = 100.*correct/total\n",
        "    device.nets[model_id]['test_acc_tracker'].append(acc)\n",
        "\n",
        "    device.nets[model_id]['net'].train()\n",
        "    return ('%.3f' % loss, '%.3f' % acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4waD2ZlQI4X2"
      },
      "source": [
        "**Implementing Components for Federated Learning**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X0fTWKg6hBY"
      },
      "source": [
        "def model_average_weight(devices, model_id):\n",
        "    '''\n",
        "    devices: a list of devices generated by create_devices\n",
        "    Returns an the average of the weights.\n",
        "    '''\n",
        "    d_id = 0\n",
        "    while(d_id < len(devices) and devices[d_id].active[model_id] == 0):\n",
        "        d_id += 1\n",
        "\n",
        "    if(d_id >= len(devices)):\n",
        "        return None\n",
        "        \n",
        "    global_tensors = copy.deepcopy(devices[d_id].nets[model_id]['net'].state_dict()) #initialize a global tensor with the weights of the first device\n",
        "    ranking_sum = devices[d_id].ranking[model_id]\n",
        "\n",
        "    for i in range(0, len(devices)):#iterate over the remaining devices\n",
        "        if(i == d_id):\n",
        "            for j in global_tensors.keys(): #add the tensors together by the key they are indexed by\n",
        "                global_tensors[j] = global_tensors[j]*devices[d_id].ranking[model_id]\n",
        "        if(devices[i].active[model_id] == 1):\n",
        "            #for easy/ less complicated referencing, store the device and the state_dict\n",
        "            d = devices[i]\n",
        "            d_tensors = d.nets[model_id]['net'].state_dict()\n",
        "            for j in global_tensors.keys(): #add the tensors together by the key they are indexed by\n",
        "                global_tensors[j] += (d_tensors[j]*d.ranking[model_id]).type_as(global_tensors[j])\n",
        "            ranking_sum += devices[i].ranking[model_id]\n",
        "\n",
        "    for j in global_tensors.keys(): #average each tensor by the number of devices\n",
        "        global_tensors[j] = global_tensors[j]/ranking_sum\n",
        "    return global_tensors #return the averaged weights\n",
        "\n",
        "\n",
        "\n",
        "def get_devices_for_round(devices):\n",
        "    '''\n",
        "    This function will select a percentage of devices to participate in each training round.\n",
        "    '''\n",
        "    #randomly choose device_pct*len(devices) devices from the devices array without replacement\n",
        "    arr = random.sample(devices, k=round(DEVICE_PCT*len(devices)))\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KNmDbPh5b0h"
      },
      "source": [
        "---\n",
        "\n",
        "### **2. Non-IID Testing and Archetype Definition Code**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yUOGnkZCbp-"
      },
      "source": [
        "**Non-iid Sampling**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VajsGz0wiL05",
        "outputId": "a84f905d-78b0-4263-c4b0-2acf79fb8aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# creates noniid TRAINING and VALIDATION datasets for each group\n",
        "def uniform_sampler(dataset, num_items_per_device, archetype, bias):\n",
        "    '''\n",
        "    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n",
        "    num_items_per_device: how many samples to assign to each device\n",
        "    archetype: a dictionary of arrays representing the labels that is predominantly represented by this edge device\n",
        "        device index -> array of archetypes\n",
        "    bias: a dictionary of the percent of samples that are represented by the archetype\n",
        "        device index -> value from 0 to 1\n",
        "\n",
        "    return: a dictionary of the following format:\n",
        "      {\n",
        "        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n",
        "        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n",
        "        ...\n",
        "      }\n",
        "\n",
        "    '''\n",
        "    #label dict stores the indexes of the dataset examples that fall into the ith group of CIFAR\n",
        "    label_dict = {}\n",
        "    for i in range(0, 10): #assuming CIFAR, which has labels 0-9\n",
        "        label_dict[i] = []\n",
        "    for i in range(len(dataset)):\n",
        "        label = dataset[i][1]\n",
        "        label_dict[label].append(i)\n",
        "    \n",
        "    final_dict = {} #final dict is to be returned\n",
        "    for i in range(NUM_DEVICES):\n",
        "        bias_group = []\n",
        "        not_bias_group = []\n",
        "        archs = [0,1,2,3,4,5,6,7,8,9] #10 archetypes\n",
        "        for j in label_dict.keys():\n",
        "            if(j in archetype[i]):\n",
        "                bias_group += label_dict[j]\n",
        "            #else:\n",
        "            if(archetype[i][0] in [0,1,2,3,4]): #two meta-archetypes\n",
        "                if(j in [0,1,2,3,4] and j != archetype[i][0]):\n",
        "                    not_bias_group += label_dict[j]\n",
        "            elif(archetype[i][0] in [5,6,7,8,9]): #two meta-archetypes\n",
        "                if(j in [5,6,7,8,9] and j != archetype[i][0]):\n",
        "                    not_bias_group += label_dict[j]\n",
        "\n",
        "        exs = random.sample(bias_group, int(num_items_per_device*bias[i]))\n",
        "        exs.extend(random.sample(not_bias_group, num_items_per_device-int(num_items_per_device*bias[i])))\n",
        "        random.shuffle(exs)\n",
        "        final_dict[i] = exs\n",
        "    return final_dict\n",
        "  \n",
        "def hypergeometric_sampler(dataset, num_items_per_device, archetype):\n",
        "    '''\n",
        "    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n",
        "    num_items_per_device: how many samples to assign to each device\n",
        "    archetype: a dictionary of arrays representing the labels that is predominantly represented by this edge device\n",
        "        device index -> array of archetypes\n",
        "\n",
        "    return: a dictionary of the following format:\n",
        "      {\n",
        "        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n",
        "        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n",
        "        ...\n",
        "      }\n",
        "\n",
        "    '''\n",
        "    #label dict stores the indexes of the dataset examples that fall into the ith group of CIFAR\n",
        "    label_dict = {}\n",
        "    for i in range(0, 10): #assuming CIFAR, which has labels 0-9\n",
        "        label_dict[i] = []\n",
        "    for i in range(len(dataset)):\n",
        "        label = dataset[i][1]\n",
        "        label_dict[label].append(i)\n",
        "  \n",
        "    final_dict = {} #final dict is to be returned\n",
        "\n",
        "    rng = np.random.default_rng()\n",
        "    \n",
        "    for i in range(NUM_DEVICES):\n",
        "        N = 110\n",
        "        ngood = archetype[i][0]*20 + 5\n",
        "        #ngood = archetype[i][0]*10 + 5\n",
        "        nbad = N - ngood\n",
        "        nsamp = 10\n",
        "        classes = rng.hypergeometric(ngood, nbad, nsamp, num_items_per_device)\n",
        "        unique_elements, counts_elements = np.unique(classes, return_counts=True)\n",
        "        # how many from each label\n",
        "        counts = dict(zip(list(unique_elements), list(counts_elements)))\n",
        "        for j in range(10):\n",
        "            if j not in counts:\n",
        "                counts[j] = 0\n",
        "        exs = []\n",
        "        for j in label_dict.keys():\n",
        "            exs.extend(random.sample(label_dict[j], counts[j]))\n",
        "\n",
        "        random.shuffle(exs)\n",
        "        final_dict[i] = exs\n",
        "    return final_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBHBZVehNZNI"
      },
      "source": [
        "---\n",
        "**Group-based Testing**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99S3opJONpeW"
      },
      "source": [
        "# creates noniid TEST datasets for each group\n",
        "def cifar_noniid_group_test(dataset):\n",
        "\n",
        "    #label dict stores the indexes of the dataset examples that fall into the ith group of CIFAR\n",
        "    label_dict = {}\n",
        "    for i in range(0, 10): #assuming CIFAR, which has labels 0-9, change later\n",
        "        label_dict[i] = []\n",
        "    for i in range(len(dataset)):\n",
        "        label = dataset[i][1]\n",
        "        label_dict[label].append(i)\n",
        "    return label_dict\n",
        "\n",
        "# gets per-group accuracy of global model\n",
        "def test_group(epoch, device, model_id, label_dict, dataset = testset):\n",
        "    \n",
        "    net = device.nets[model_id]['net']\n",
        "    net.eval() #turn the net into evaluaton mode\n",
        "    with torch.no_grad():\n",
        "        #for group in label_dict.keys(): \n",
        "        for group in [0,1,2,3,4,5,6,7,8,9]: #10 archetypes\n",
        "            test_loss, correct, total = 0, 0, 0\n",
        "            new_dataset = DatasetSplit(dataset, label_dict[group])\n",
        "            dataloader = torch.utils.data.DataLoader(new_dataset, batch_size=128, shuffle=False,\n",
        "                                            num_workers=2)\n",
        "            for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                test_loss += loss.item()\n",
        "                #print(\"test_group\", loss.item())\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "            # Compute and print loss and accuracy at the end of the group\n",
        "            loss = test_loss / (batch_idx + 1)\n",
        "            acc = 100.* correct / total\n",
        "\n",
        "            outputs = [0]\n",
        "            test_loss = 0\n",
        "    net.train()\n",
        "    sys.stdout.flush()  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9KiAYyHS8VA"
      },
      "source": [
        "## Code Cell 3.1\n",
        "\n",
        "def quantizer(input, nbit):\n",
        "    '''\n",
        "    input: full precision tensor in the range [0, 1]\n",
        "    returns a quantized tensor\n",
        "    '''\n",
        "    scale_factor = 1 / (2**nbit -  1)\n",
        "\n",
        "    # scale input by inverse of scale_factor and round to nearest integer\n",
        "    output = input / scale_factor\n",
        "    output = torch.round(output)\n",
        "\n",
        "    # scale rounded output back and return\n",
        "    output *= scale_factor\n",
        "    return output\n",
        "\n",
        "def quantize_model(model, nbit):\n",
        "    '''\n",
        "    Quantizes the ConvNet model.\n",
        "    '''\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "            m.weight.data, m.adaptive_scale = dorefa_g(m.weight, nbit)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data,_ = dorefa_g(m.bias, nbit, m.adaptive_scale)\n",
        "\n",
        "def dorefa_g(w, nbit, adaptive_scale=None):\n",
        "    '''\n",
        "    w: a floating-point weight tensor to quantize\n",
        "    nbit: the number of bits in the quantized representation\n",
        "    adaptive_scale: the maximum scale value. if None, it is set to be the\n",
        "                    absolute maximum value in w.\n",
        "    '''\n",
        "    if adaptive_scale is None:\n",
        "        adaptive_scale = torch.max(torch.abs(w))\n",
        "    \n",
        "    # follows equations above\n",
        "    sigma = torch.rand(w.shape) - 0.5\n",
        "    noise = sigma / (2**nbit - 1)\n",
        "    # avoid type errors\n",
        "    noise = noise.type(w.type())\n",
        "    inp = w / (2*adaptive_scale) + 0.5 + noise\n",
        "    w_q = 2*adaptive_scale * (quantizer(inp, nbit) - 0.5)\n",
        "\n",
        "    return w_q, adaptive_scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtkYyNF5Oo9J"
      },
      "source": [
        "---\n",
        "**Federated Learning Results in Non-IID Setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKogIgMUgE0f"
      },
      "source": [
        "# Train model on each device\n",
        "# Get rankings on each device\n",
        "# Update weights\n",
        "\n",
        "net = ConvNet().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "devices_archetype = [[i//3] for i in range(NUM_DEVICES)]\n",
        "devices_bias = [random.uniform(0.6, 0.7) for i in range(NUM_DEVICES)]\n",
        "\n",
        "'''\n",
        "device 0-2: 85% 0, 15% 1,2\n",
        "device 3-5: 85% 1, 15% 0,2\n",
        "device 6-8: 85% 2, 15% 0,1\n",
        "device 9-11: 85% 3, 15% 4,5\n",
        "device 12-14: 85% 4, 15% 3,5\n",
        "device 15-17: 85% 5, 15% 3,4\n",
        "'''\n",
        "\n",
        "if SAMPLER == \"uniform_bias\":\n",
        "    data_sampler = uniform_sampler\n",
        "elif SAMPLER == \"hypergeometric\":\n",
        "    data_sampler = hypergeometri_sampler\n",
        "\n",
        "\n",
        "train_idxs = data_sampler(trainset, NUM_TRAIN_PER_DEVICE, devices_archetype, devices_bias)\n",
        "valid_idxs = data_sampler(validset, NUM_VALID_PER_DEVICE, devices_archetype, devices_bias)\n",
        "test_idxs_device = data_sampler(testset, NUM_TEST_PER_DEVICE, devices_archetype, devices_bias)\n",
        "\n",
        "label_dict_test = cifar_noniid_group_test(testset)\n",
        "# test_idxs = label_dict_test[0] + label_dict_test[1]\n",
        "test_idxs = []\n",
        "for i in range(0, NUM_LABELS):\n",
        "    test_idxs += label_dict_test[i] \n",
        "random.shuffle(test_idxs)\n",
        "\n",
        "arch_testset = DatasetSplit(testset, test_idxs)\n",
        "test_dataloader = torch.utils.data.DataLoader(arch_testset, batch_size=128,\n",
        "                                                shuffle=True, num_workers=2)\n",
        "\n",
        "label_dict_valid = cifar_noniid_group_test(validset)\n",
        "\n",
        "#print(devices_archetype)\n",
        "#print(data_idxs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnf7JCFHUQPI",
        "outputId": "07eab11d-fd78-47fe-a90a-7d9d769ed1f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "#FedCD: Federated Cloning and Deletion\n",
        "\n",
        "if FEDCD:\n",
        "\n",
        "    model_id_lst = [0]\n",
        "\n",
        "    ## Device creation\n",
        "    devices = create_devices(net, trainset, validset, testset, train_idxs, valid_idxs, test_idxs_device, devices_bias, devices_archetype)\n",
        "    print('Devices', len(devices))\n",
        "\n",
        "    start_time = time.time()\n",
        "    for round_num in range(NUM_ROUNDS):\n",
        "      \n",
        "        round_devices = get_devices_for_round(devices)\n",
        "\n",
        "        for device in round_devices:\n",
        "            for model_id in model_id_lst:\n",
        "                if(device.active[model_id] != 0):\n",
        "                    # Training\n",
        "                    for local_epoch in range(NUM_LOCAL_EPOCHS):\n",
        "                        train(local_epoch, device, model_id) \n",
        "                        # print(\"Device: \" + str(device.idx) + \" VALIDATION: model_id # \" + str(model_id))\n",
        "                        # validate(local_epoch, device, model_id) \n",
        "                        # print()\n",
        "                        # test_group(round_num, device, model_id, label_dict_valid, validset)   \n",
        "                    # after training, quantize the learned model\n",
        "                    if QUANTIZE:\n",
        "                        quantize_model(device.nets[model_id]['net'], NBIT)\n",
        "            \n",
        "        for model_id in model_id_lst:\n",
        "            w_avg = model_average_weight(round_devices, model_id)\n",
        "\n",
        "            if(w_avg != None):\n",
        "                for device in devices:\n",
        "                    if(device.active[model_id]!= 0):\n",
        "                        device.nets[model_id]['net'].load_state_dict(w_avg)\n",
        "                        device.nets[model_id]['optimizer'].zero_grad()\n",
        "                        device.nets[model_id]['optimizer'].step()\n",
        "                        device.nets[model_id]['scheduler'].step()\n",
        "                \n",
        "            # test accuracy with highest ranking model\n",
        "            if((devices[0].active[model_id] == 1) and (devices[0].ranking[model_id] == max(devices[0].ranking))):\n",
        "                # print()\n",
        "                # print(\"ALL-TEST ACCURACY\")\n",
        "                test(round_num, devices[0], model_id, arch_testset, test_dataloader)\n",
        "                # print(\"ALL-TEST TEST GROUPS ACCURACY\")\n",
        "                # test_group(round_num, devices[0], model_id, label_dict_test)\n",
        "        \n",
        "        # Validation\n",
        "        if round_num not in DUPLICATE_MILESTONES:\n",
        "            for device in round_devices:\n",
        "                for model_id in model_id_lst:\n",
        "                    if(device.active[model_id] != 0):\n",
        "                        validate(NUM_LOCAL_EPOCHS - 1, device, model_id)    # <- there are print statements here\n",
        "                # Figure out rankings here\n",
        "                device.update_ranking()\n",
        "\n",
        "        # Testing with IID data from device\n",
        "        \n",
        "        test_iid_results = []\n",
        "        for index in range(len(devices)):\n",
        "            device = devices[index]\n",
        "            max_model = device.ranking.index(max(device.ranking))\n",
        "            test_iid_results.append(float(test(round_num, device, max_model, device.device_testset, device.testloader)[1]))\n",
        "\n",
        "        print(round_num, test_iid_results)\n",
        "        \"\"\"\n",
        "        active_arr_tracker = [sum(devices[i].active) for i in range(len(devices))]\n",
        "        print(round_num, active_arr_tracker)\n",
        "        for i in range(len(devices)):\n",
        "            print(i, \" \", devices[i].ranking.index(max(devices[i].ranking)), \" \", devices[i].ranking)\n",
        "        \"\"\"\n",
        "\n",
        "        #duplicate all models\n",
        "        if(round_num in DUPLICATE_MILESTONES):\n",
        "            # Run validation and update rankings for everyone\n",
        "            for device in devices:\n",
        "                for model_id in model_id_lst:\n",
        "                    if(device.active[model_id] != 0):\n",
        "                        validate(NUM_LOCAL_EPOCHS - 1, device, model_id)    # <- there are print statements here\n",
        "                # Figure out rankings here\n",
        "                device.update_ranking()\n",
        "\n",
        "            # Number of nets to duplicate\n",
        "            nets_to_create = len(model_id_lst)\n",
        "            for model_id in range(0, nets_to_create):\n",
        "                for device in devices:\n",
        "                    # If model wasn't already removed\n",
        "                    if device.active[model_id] != 0:    \n",
        "                        device_net = ConvNet().cuda()\n",
        "                        device_net.load_state_dict(device.nets[model_id]['net'].state_dict())\n",
        "                        valid_loss_tracker = [copy.deepcopy(device.nets[model_id]['valid_loss_tracker'][-1])]\n",
        "                        valid_acc_tracker = [100 - copy.deepcopy(device.nets[model_id]['valid_acc_tracker'][-1])]\n",
        "                        optimizer = torch.optim.SGD(device_net.parameters(), lr=0.1, momentum=0.9,\n",
        "                                                    weight_decay=5e-4)\n",
        "                        rounds_passed = len(device.nets[model_id]['train_acc_tracker'])\n",
        "                        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                                        milestones=[25-rounds_passed, 50-rounds_passed, 75-rounds_passed],\n",
        "                                                                        gamma=0.1)\n",
        "                        device.nets.append({\n",
        "                            'net': device_net,\n",
        "                            'optimizer': optimizer,\n",
        "                            'scheduler': scheduler,\n",
        "                            'train_loss_tracker': [],\n",
        "                            'train_acc_tracker': [],\n",
        "                            'valid_loss_tracker': valid_loss_tracker,\n",
        "                            'valid_acc_tracker': valid_acc_tracker,\n",
        "                            'test_loss_tracker': [],\n",
        "                            'test_acc_tracker': [],\n",
        "                        })\n",
        "                        device.active.append(1)\n",
        "                        # Heavily rank the devices that are underperforming for new models and vice versa\n",
        "                        if len(valid_acc_tracker) > 0:\n",
        "                            device.update_ranking(removed = False, duplicate_model_id = model_id + nets_to_create, offset_rank = valid_acc_tracker[-1])\n",
        "                        else:\n",
        "                            device.update_ranking()\n",
        "                    # If model was already removed\n",
        "                    else:                           \n",
        "                        device.nets.append({\n",
        "                            'valid_acc_tracker': [0.],\n",
        "                        })\n",
        "                        device.active.append(0)\n",
        "                        device.update_ranking(removed = True, duplicate_model_id = model_id + nets_to_create)\n",
        "\n",
        "                model_id_lst.append(model_id + nets_to_create)\n",
        "        # print('model id list:', model_id_lst)\n",
        "        # print('best model:', [device.ranking.index(max(device.ranking)) for device in devices])\n",
        "        # for device in devices:\n",
        "            # print(\"device's active models:\", device.active)\n",
        "            # print(\"device \" + str(device.idx) + \" ranking: \" + str(device.ranking))\n",
        "            # print(\"archetype:\", device.archetype)\n",
        "\n",
        "        # TODO save both print after\n",
        "\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print('Total training time: {} seconds'.format(total_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Devices 30\n",
            "0 [7.0, 9.2, 8.6, 10.0, 8.0, 8.4, 60.6, 10.8, 64.8, 64.8, 66.2, 69.2, 9.2, 8.2, 6.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "1 [0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 16.2, 14.6, 12.8, 16.6, 15.8, 67.4, 63.2, 13.2, 9.0, 10.2, 12.0, 15.4, 9.2, 8.0]\n",
            "Total training time: 200.32439589500427 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfooNMNyTcte",
        "outputId": "1a633f0f-b51f-4dae-eb3a-630bd06e203e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "# FedAvg: Federated Learning Baseline\n",
        "\n",
        "if FEDAVG:\n",
        "\n",
        "    model_id_lst = [0]\n",
        "\n",
        "    ## Device creation\n",
        "    devices = create_devices(net, trainset, validset, testset, train_idxs, valid_idxs, test_idxs_device, devices_bias, devices_archetype)\n",
        "    print('Devices', len(devices))\n",
        "\n",
        "    start_time = time.time()\n",
        "    for round_num in range(NUM_ROUNDS):\n",
        "      \n",
        "        round_devices = get_devices_for_round(devices)\n",
        "\n",
        "        for device in round_devices:\n",
        "            for model_id in model_id_lst:\n",
        "                if(device.active[model_id] != 0):\n",
        "                    # Training\n",
        "                    for local_epoch in range(NUM_LOCAL_EPOCHS):\n",
        "                        train(local_epoch, device, model_id) \n",
        "                        # print(\"Device: \" + str(device.idx) + \" VALIDATION: model_id # \" + str(model_id))\n",
        "                        # validate(local_epoch, device, model_id) \n",
        "                        # print()\n",
        "                        # test_group(round_num, device, model_id, label_dict_valid, validset)   \n",
        "                    # after training, quantize the learned model\n",
        "                    if QUANTIZE:\n",
        "                        quantize_model(device.nets[model_id]['net'], NBIT)\n",
        "            \n",
        "        for model_id in model_id_lst:\n",
        "            w_avg = model_average_weight(round_devices, model_id)\n",
        "\n",
        "            if(w_avg != None):\n",
        "                for device in devices:\n",
        "                    if(device.active[model_id]!= 0):\n",
        "                        device.nets[model_id]['net'].load_state_dict(w_avg)\n",
        "                        device.nets[model_id]['optimizer'].zero_grad()\n",
        "                        device.nets[model_id]['optimizer'].step()\n",
        "                        device.nets[model_id]['scheduler'].step()\n",
        "                \n",
        "            # test accuracy with highest ranking model\n",
        "            if((devices[0].active[model_id] == 1) and (devices[0].ranking[model_id] == max(devices[0].ranking))):\n",
        "                # print()\n",
        "                # print(\"ALL-TEST ACCURACY\")\n",
        "                test(round_num, devices[0], model_id, arch_testset, test_dataloader)\n",
        "                # print(\"ALL-TEST TEST GROUPS ACCURACY\")\n",
        "                # test_group(round_num, devices[0], model_id, label_dict_test)\n",
        "        \n",
        "        # Validation\n",
        "        if round_num not in DUPLICATE_MILESTONES:\n",
        "            for device in round_devices:\n",
        "                for model_id in model_id_lst:\n",
        "                    if(device.active[model_id] != 0):\n",
        "                        validate(NUM_LOCAL_EPOCHS - 1, device, model_id)    # <- there are print statements here\n",
        "                # Figure out rankings here\n",
        "                device.update_ranking()\n",
        "\n",
        "        # Testing with IID from device\n",
        "        test_iid_results = []\n",
        "        for index in range(len(devices)):\n",
        "            device = devices[index]\n",
        "            max_model = device.ranking.index(max(device.ranking))\n",
        "            test_iid_results.append(float(test(round_num, device, max_model, device.device_testset, device.testloader)[1]))\n",
        "\n",
        "        print(round_num, test_iid_results)\n",
        "        \"\"\"\n",
        "        active_arr_tracker = [sum(devices[i].active) for i in range(len(devices))]\n",
        "        print(round_num, active_arr_tracker)\n",
        "        for i in range(len(devices)):\n",
        "            print(i, devices[i].ranking)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print('Total training time: {} seconds'.format(total_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Devices 30\n",
            "0 [10.8, 10.4, 7.2, 9.4, 9.8, 9.0, 12.0, 10.4, 8.4, 7.8, 6.8, 8.6, 67.0, 6.8, 69.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "1 [8.2, 9.2, 10.4, 8.8, 8.0, 7.8, 8.0, 10.8, 9.2, 64.8, 66.2, 69.2, 6.4, 8.2, 7.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0]\n",
            "Total training time: 200.23145079612732 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}